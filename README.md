## NMT_KoBERT
This repo covers Korean-English Neural Machine Translation. For this task, three models are used. Each is **Transformer**, **NMT_BERT**, **NMT_MultiBERT**, **NMT_KoBERT**. Except for Transformer, all three models utilize pre-trained models. BERT_base is a model trained based on English sentences, and BERT_Multilingual is trained based on multiple language data. And KoBERT was trained with Korean data. The point of observation is how well each model handles Korean.

<br>
<br>

## Models
**Transformer**
> In here, Transformer model is Vanilla Transformer model which proposed in "Attention is all you need" paper. <br> The model is used as baseline model in this entire experiment.

<br>

**NMT_BERT**
> The model use a pre-trained BERT as an encoder in the Transformer-based Encoder-Decoder architecture. <br> As for the method of utilizing BERT, the method suggested in the "Apply BERT on Something" paper was used.

<br>

**NMT_MultiBERT**

<br>

**NMT_KoBERT**

<br>
<br>

## Configurations
> Model Configs

<br>

> Training Configs

<br>
<br>

## Result

<br>
<br>

## Reference
* Attention Is All You Need
